% LaTeX file for resume 
% This file uses the resume document class (res.cls)

\documentclass{res} 
%\usepackage{helvetica} % uses helvetica postscript font (download helvetica.sty)
%\usepackage{newcent}   % uses new century schoolbook postscript font 
\setlength{\textheight}{10in} % increase text height to fit on 1-page 
%\usepackage{setspace}
%\singlespacing
%\onehalfspacing
%\usepackage{fullpage}
\usepackage[top=1in, bottom=1in, left=0.5in, right=1.5in]{geometry}

\begin{document} 

\name{Jia Jianfeng\\[12pt]}     % the \\[12pt] adds a blank
				        % line after name      

%\address{\bf  ADDRESS\\Donald Bren Hall, Room 3019\\University of California,Irvine\\Irvine,CA 92967}
\address{jianfeng.jia@gmail.com (949) 678-9893}
                                  
\begin{resume}

\section{JOB OBJECTIVE}          
    Seeking a \emph{Summer Internship} position.

\section{WORK EXPERIENCE}
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Software Engineer Intern} \>SRCH2.com, Irvine, US     \>Jun.2013 - Sept.2013\\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Implementing a Java SDK for the C++ search engine library. Implementing a Android search app by this SDK.
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Research Associate} \>Sogou.com,Beijing, China     \>Jul.2008 - Jul.2012\\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Working at \emph{Chinese Input Method Editor} (IME) Research Group, 
   working on the Terabytes scale of data to improve the precision of IME product.
\section{EDUCATION}          
%    Ph.D. student of Computer Science, University of California,Irvine,from Sept.2012\\        
%    M.S. Computer Science, Xiamen University, Xiamen,China, July 2008\\
%        Thesis:\emph{The Application of Dependency Grammar in Chinese-to-English Statistical Machine Translation}\\
%    B.S. Computer Science, Xiamen University, Xiamen,China, July 2005\\       
   \vspace{-0.1in}	
   \begin{tabbing}
  \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Ph.D student,Computer Science} \>  University of California,Irvine,CA,US \>Sept.2012 - Present\\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Research interest:\emph{ Parallel Computing, Large scale data processing }
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf M.S. Computer Science} \>Xiamen University,China     \>Sept.2005 - Jul.2008\\
%                             \>Xiamen,China
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
%   Thesis:\emph{The Application of Dependency Grammar in Statistical Machine Translation}
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf B.S. Computer Science} \>Xiamen University,China     \>Seps.2001 - Jul.2005\\
%                             \>Xiamen,China
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
 
\section{SKILLS}          
    C++,Java,JNI,Android,Python, Hadoop,Hbase,Pig.
 
\section{PROJECTS}
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Genome assembling project using Hyracks }\> \>UCI     \\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Using the Hyracks platform( Parallel data processing system competing with Hadoop) to build the genome graph which contains billions of nodes. And we achieved 30\% performance improvement.
%
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Feedback Data flow System of IME using HBase and Pig}\> \>Sogou.com     \\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Building the automatic feedback data processing system using HBase to store 30G data per day, 15T data totally. Using Pig to analyze and explore the global user behavior and also keeping track of single user's daily statistical features. 
%   Through this analyse tool, we found some certain flaw of the product and provided the solution to improve the precision.
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Large-scale Language Model(LM) for Cloud IME} \> \>Sogou.com     \\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Building the automatic process of LM updating weekly from the 500G new corpus using Hadoop platform.Building the decoder for trigram LM and the re-rank model to improve the precision, which is 3\% higher than competitors' products.
   \vspace{-0.1in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Automatic New Word Detection}\> \>Sogou.com     \\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Devising a novel approach of New Word Detection system based on Entropy-loss theory on Hadoop platform.\\
%   This system do not need any Linguistics dictionary, fully base on the statistical information of corpus itself.
%   The LM building on those new words improved 1\% precision in desktop IME comparatively to those on normal linguistic words.
   \vspace{-0.2in}	
   \begin{tabbing}
   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
    {\bf Dependency Treelet Based Chinese-to-English SMT System} \> \>Xiamen University\\
   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
   Devising two dependency structure based statistical machine translation system prototypes. 
%   Model 1 was completely lexicalized; we extract the treelet structure in the source language side and the continuous corresponding string of words in the target language side\\
%   Model 2 applied the generalization to summarize the learned lexical template. Different from the before systems, we applied grammar labels to constrain the generalized template. And the Bleu score of Model 2 was apparent higher than the Model1 and at the same level with the Pharaoh.
%   \begin{tabbing}
%   \hspace{2.5in}\= \hspace{3in}\= \kill % set up two tab positions
%    {\bf Shift-Reduced Dependency Parser} \> \>Xiamen University\\
%   \end{tabbing}\vspace{-30pt}      % suppress blank line after tabbing
%   Developing an action sequence based deterministic parser.
%   We achieved dependence arc marker accuracy rate (LAS) 76.36\% on Chinese and 82.93\% on the English on the benchmark set in CoNLL2007.


%\section{PATENTS}
%\emph{A Input Method In The Hardware Device}, \textbf{Jianfeng Jia}, Yanfeng Wang, Yang Zhang, CN102087550A public on Jun.8, 2011\\
%\emph{A Method And A System Providing New Words And Hot Words}, \textbf{Jianfeng Jia}, Yang Zhang, Yanfeng Wang, CN102163198A, public on Aug.24, 2011 \\



%\section{HONORS AND AWARDS}             
%    \begin{itemize}
%     Rhinoceroses prize for building the trigram and re-rank model for cloud IME, Sogou Research, 2010 \\
%     Rhinoceroses prize for innovation on improve 20\% precision rate on the long sentence test set of desktop IME, Sogou Research,2009 \\
%     First-Class Scholarship, Xiamen University, 2005 \\
%     First-Class Scholarship, Xiamen University, 2004 
%    \end{itemize}

%\section{PUBLICATIONS}
%\begin{itemize}
%     \textbf{Jianfeng Jia}, \emph{The Application of Statistical Language Model in Sogou Pinyin Input Method Editor}, Journal of Chinese Association for Artificial Intelligence 2011.vol.1 (4).\\
%     \textbf{Jianfeng Jia}, Xiaodong Shi, Xingbang Lai, \emph{HMM-based Chinese Pinyin Input Method}, J. Modern Computer 2008. (4) 4-6.\\
%     Xiaodong Shi, Yidong Chen, \textbf{Jianfeng Jia}, \emph{Dependency-Based Chinese-English Statistical Machine Translation}, Conference on Intelligent Text Processing and Computational Linguistics (CICLing) 2007, Mexico City, Mexico.\\
%     \textbf{Jianfeng Jia}, Xiaodong Shi, Yu Chen, \emph{Shift-Reduce Algorithm and Structure Model Based Dependency Statistical Parser}, International Chinese Computing Conference (ICCC) 2007, Wuhan, China.
%\end{itemize}

\end{resume}
\end{document}
